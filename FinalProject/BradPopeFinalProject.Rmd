---
title: "Item Profiling and Forecasting - Time Series Class"
author: "Brad Pope"
date: "April, 2019"
output: html_document
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r Import, include=FALSE, echo=FALSE}

library(readr)
library(xts)
library(astsa)
library(quantmod)

cleanitem <- read_delim("C:/1 Brad/Dropbox/Data Science/Time Series Analysis/cleanitem.txt",                         "\t", escape_double = FALSE, col_types = cols(Date = col_date(format = "%m/%d/%Y")), trim_ws = TRUE)

# second dataset was a late add due to the additional grading criteria on submission page.  If known upfront this could easily be a single dataset
cleanitem2 <- read_delim("C:/1 Brad/Dropbox/Data Science/Time Series Analysis/cleanitem2.txt", 
                         "\t", escape_double = FALSE, col_types = cols(Date = col_date(format = "%m/%d/%Y"), 
                          Month = col_integer(), YearQTR = col_integer()), trim_ws = TRUE)


include=TRUE
#Converting the data to an xts
item <- xts(cleanitem$`Total volume`, order.by=as.Date(cleanitem$`Date`, "%m/%d/%Y"))

#Converting the data to a time series
c.item <- ts(cleanitem, frequency=365, start=c(2016, 268)) 


```
## Summary of the Dataset :

####This is the sales history for a single UPC for a non-perishable product sold in a single retail location.  The data was captured by a retailer when shoppers purchased the product. I chose this dataset as it is based on a relevant real world product to my company and I started working on the project before it was recommended we use data from the astsa package.

####Here are the start, end, frequency and standard deviation:

Start Year and Day: `r start(c.item)`

End Year and Day:  `r end(c.item)`

Frequency (days in a year since daily): `r frequency(c.item)`

Standard Deviation of the sales amount: `r sd(c.item)`


## Exploration

####Exploration of the dataset will take place with a trend, histogram and boxplots.  Since there is only one variable there is no need to do scatterplot.

```{r plot, echo=FALSE, warning=FALSE}
plot(item)
abline( lm((item) ~ index(item) ), col="light blue", lwd=3 )  
```

####The plot over time doesn't show a clear linear increase or decrease. There is variability that happens each week with some days that have zero sales (Major holidays or out of stocks).


```{r hist, echo=FALSE}
hist(item)
```

####The histogram reveals a mostly normal distribution though we can see the zero and very low volume days on the left hand most bar.  These are valid data points and should be left in.

```{r Boxplot, echo=FALSE}

attach(cleanitem2)
par(mfrow=c(2,2))
boxplot(cleanitem2$`Total volume` ~ cleanitem2$'Month', main="Boxplot by Month")
boxplot(cleanitem2$`Total volume` ~ cleanitem2$'Year', main="Boxplot by Year")
boxplot(cleanitem2$`Total volume` ~ cleanitem2$'Quarter', main="Boxplot by Season")
boxplot(cleanitem2$`Total volume` ~ cleanitem2$'YearQTR', main="Boxplot by Year and Quarter")

```

####The set of boxplots show some of the potential differences by month, quarter and year.  The inner quartile range of the boxes overlaps on many of the comparisons.  While we do see more variation in some time periods with some YearQtr and month combinations selling more than others. 

## Decomposition
####Decomposition stage (plotting)

```{r, Decompose, echo=FALSE}
c.itemdecomp <- decompose(c.item, type =c("additive"),filter = NULL)
plot(c.itemdecomp)
```

####I chose additive because the seasonal and irregular components to not change despite the changes in the trend level.

####The decomposition function was didn't work quite the same as it did on the astsa package.  I spent some time troubleshooting reviewing my dataset multiple times.  There is a value for every day and, though there are days with zero sales, these are real occurrences so they should be included.  Decompose may not be the best function for this data set.  As such, the box plots above go a level deeper within each time period.

## Regression and Transformation
####Building the model, evaluation and interpretation
####The first step in the regression was to take the difference using the "diff" function to remove any trending and reexamine the ACF and PACF. Note: since the data didn't scale up or down there was no need for the log (tested but didn't yeild better results)

```{r, Regression, echo=FALSE, warning=FALSE}
#removed the trend
d_item=diff(item)
acf(d_item, lag.max = 35, plot = TRUE, na.action = na.contiguous, demean = TRUE)
pacf(d_item, lag.max = 35, plot = TRUE, na.action = na.contiguous, demean = TRUE)
plot(d_item)
```

####As we can see from the ACF graph both the ACF and PACF trail off which indicate a ARMA model where p = 1, and q = 1.  Also,  the time series of the differenced data exhibits some ARIMA behavior so "d"" was set equal to 1 as well.  In testing multiple values 1 was the most effective.

####In addition, when we look at the ACF we see that every seven days there is a spike indicating weekly seasonality.  This makes sense because sales spikes are commonplace on Saturdays.

####Next step is to remove the 7 day cyclical nature with diff(d_item,lag=7) and plot the ACF and PACF again.

``` {r, Regression2, echo=FALSE, warning=FALSE}
#removing the cyclical nature of the data
dd_item=diff(d_item,lag=7)

#Plot the ACF and PACF
acf(dd_item, lag.max = 60, plot = TRUE, na.action = na.contiguous, demean = TRUE)
pacf(dd_item, lag.max = 60, plot = TRUE, na.action = na.contiguous, demean = TRUE)

```


####When the 7 day lag is controlled the ACF and PACF can be reviewed on each seasonal marker (each 7 days).  When completed we see an ACF of 1 and PACF trailing off which represents a SMA(1) with a P = 0 and Q = 1.  After experimentation D was set equal to 1 as well.

#### Based on those parameters here is the fit of the regression.

```{r, SARIMA model, echo=FALSE}
#Model a Seasonal ARIMA
x=sarima(item,1,1,1,0,1,1,7,details=FALSE)
x$ttable

```

#### Since the P Values for ar1, ma1 and smal are less than .01 we can be 99% confident that they are significant and contributing to the model's performance.  There are additional model diagnostics below.


## Model Diagnostics

####Next we'll review more of Diagnostics to ensure a proper fit of the model. All of the diagnostics are printed below with a summary of the key pieces below. 


```{r, Model Diagnostics, echo = TRUE}

sarima(item,1,1,1,0,1,1,7)

```

####First review the Graphs of the Residual Analysis
##### Standardized residuals - There were no obvious patterns in the residuals which is positive
##### Sample ACF of Residuals - With the "wideness"" of the data, 95% of the data points between the two lines
##### Normal Q-Q plot - In the normality assessment extreme values there are no major departures from the line so the normal assumption here is reasonable
##### Q-statistic P Values - With another wideness assessment the points are above the line so an assumption can be made that the noise is white.  

#### The AIC and BIC values are also relatively small.  To ensure the best fit over a dozen other models were run but all had higher AIC and BIC values indicating that the model derived from the ACF and PACF was sound.

## Model Forecast
#### Since the model represents the underlying dataset well, we can then use sarima.for against the same model to forecast sales.
```{r, Forecast, echo = FALSE}
#Forecast the seasonal ARIMA
sarima.for(item,35,1,1,1,0,1,1,7)
```
#### Here we see the next 35 periods beyond the known actual sales. The daily seasonality of the data is included in the forecast.

#### This concludes the summary, exploratory, decomposition, regression, modeling, diagnostics and forecasting of the product information for the final project.

##Thanks for instructing the Time Series class!
